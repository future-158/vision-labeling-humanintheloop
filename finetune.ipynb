{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/future-158/vision-labeling-humanintheloop/blob/master/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQJ2sxNg6w1P",
        "outputId": "798a47e8-e765-4c62-b674-4a885e1cf283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv --quiet\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import dotenv\n",
        "import os\n",
        "# drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89aK_1m4chbL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDa1dRhKmOD0"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/project/buzzni')\n",
        "!make install\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570,
          "referenced_widgets": [
            "12e5a7ddb2824c5b84ffaaeecd1d3724",
            "6e4800d66aec41bc9ebc92203d1c2c23",
            "b3b09fb26d3b4c4c8a0e6ae8179cf915",
            "ce0cfa0cc607498592989082dfb2ef0f",
            "fcd497681f7d4bfe9e49b60e8a271252",
            "e1d179b1797b45e9805885330aa079c8",
            "e2ea302ea9f74abd83376ab35e968ac8",
            "5f6b1418dcf54e7f874a399d6d0b323b",
            "f05e0d8dd82745869328d6707c00d556",
            "505e22644f0c406cac79d58f0f5bf546",
            "db7defadeb6844a08b9374e690d1c237"
          ]
        },
        "id": "nwu2H5JGtJHV",
        "outputId": "49649fc1-78e0-43a1-9ddd-f8c895ee66a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 149,620,737\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.41s/it]\n",
            "100%|██████████| 4/4 [00:11<00:00,  2.98s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.01s/it]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12e5a7ddb2824c5b84ffaaeecd1d3724",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.36s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-705254a8143d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;31m# save untrained result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0meval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0mtest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     history.append({\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-100-705254a8143d>\u001b[0m in \u001b[0;36meval_epoch\u001b[0;34m(datasets, model, human_in_the_loop)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m           \u001b[0mimage_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m           \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m           \u001b[0mimage_features\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
          ]
        }
      ],
      "source": [
        "#pip install ftfy regex tqdm datasets transformers omegaconf git+https://github.com/openai/CLIP.git optuna\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import tqdm\n",
        "from datasets import concatenate_datasets, load_dataset, load_metric, load_from_disk\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from pkg_resources import packaging\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import (ConfusionMatrixDisplay, accuracy_score,\n",
        "                             confusion_matrix, precision_recall_fscore_support,\n",
        "                             top_k_accuracy_score)\n",
        "from sklearn.preprocessing import normalize\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import CIFAR100\n",
        "import optuna\n",
        "import clip\n",
        "\n",
        "cfg = OmegaConf.load('clip_config.yaml')\n",
        "\n",
        "API_VERSION= 102\n",
        "exp_name= f'clip_finetune_human_v{API_VERSION}'\n",
        "\n",
        "catalog = OmegaConf.create(\n",
        "    dict(\n",
        "    catalog=\"./dataset/data/catalog.csv\",\n",
        "    image_folder=\"./dataset/data\",\n",
        "    trial_result=\"result/${exp_name}_trial_result.csv\",\n",
        "    trials_dataframe=\"result/${exp_name}_trials_dataframe.csv\",\n",
        "    dataset_path=\"data/clip_dataset\",\n",
        "        )\n",
        ")\n",
        "\n",
        "USE_PUBLIC_DATASET = False\n",
        "THRESHOLD = 0.8\n",
        "K = 5\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 256\n",
        "PREP_BATCH_SIZE = 256\n",
        "HUMAN_IN_THE_LOOP = True\n",
        "UPDATE_SIZE = 10\n",
        "WARMUP_EPOCH = 0\n",
        "MODEL_NAME = \"ViT-B/16\"\n",
        "\n",
        "DEBUG = True\n",
        "\n",
        "\n",
        "clip.available_models()\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(MODEL_NAME,device=device,jit=False) #Must set jit=False for training\n",
        "model.eval()\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "\n",
        "id2txt = (\n",
        "    pd.read_csv(catalog.catalog, index_col=['index'])\n",
        "    ['en_final']\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "id2txt = {index : f\"This is a photo of a {txt}\" for index,txt in id2txt.items()}\n",
        "text_tokens = clip.tokenize(id2txt.values()).to(device)\n",
        "class_ids = list(range(50))\n",
        "\n",
        "if Path(catalog.dataset_path).exists():\n",
        "    ds = load_from_disk(catalog.dataset_path)\n",
        "\n",
        "else:\n",
        "    ds = load_dataset(\"imagefolder\", data_dir=catalog.image_folder, split='train')\n",
        "    test_ds = load_dataset(\"imagefolder\", data_dir=catalog.image_folder, split='test')\n",
        "\n",
        "    # true labels\n",
        "    LABELS = ds.features['label'].names\n",
        "\n",
        "    new_column = [id2txt[index] for index in ds['label']]\n",
        "    ds = ds.add_column(\"txt\", new_column)\n",
        "\n",
        "    new_column = [id2txt[index] for index in test_ds['label']]\n",
        "    test_ds = test_ds.add_column(\"txt\", new_column)\n",
        "\n",
        "    # set evaluation set as 10% train_dir\n",
        "    ds = ds.shuffle(0).train_test_split(test_size=0.2)\n",
        "    ds['eval'] = ds['test'] # rename to eval\n",
        "\n",
        "    # after manually labeling, assign 20% of unlabel folder to test_set\n",
        "    # use rest of data as unlabel dataset.\n",
        "\n",
        "    # test_ds = test_ds.train_test_split(shuffle=True, seed=0, test_size=0.2)\n",
        "    # ds['test'] = test_ds['test'] # 20%\n",
        "\n",
        "    # empty dataset. iterative add datas.\n",
        "    ds['test'] = test_ds.filter(lambda x: False)\n",
        "    ds['unlabel'] = test_ds\n",
        "\n",
        "    def transform(example):\n",
        "        example['image'] = preprocess(example['image'].resize((224,224)).convert('RGB'))\n",
        "        return example\n",
        "\n",
        "    def transforms(examples):\n",
        "        # examples[\"pixel_values\"] = [jitter(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "        examples['image'] =  [preprocess(image.resize((224,224)).convert('RGB')) for image in examples['image']]\n",
        "        return examples\n",
        "    # ds = ds.map(transforms, batched=True, batch_size=PREP_BATCH_SIZE)\n",
        "    ds = ds.map(transforms, batched=False)\n",
        "    ds.save_to_disk(catalog.dataset_path)\n",
        "\n",
        "if DEBUG:\n",
        "    ds['train'] = ds['train'].select(range(100))\n",
        "    ds['eval'] = ds['eval'].select(range(100))\n",
        "    ds['unlabel'] = ds['unlabel'].select(range(100))\n",
        "    ds['test'] = ds['test'].select(range(100))   \n",
        "\n",
        "\n",
        "def compute_metrics(labels, probs):\n",
        "    # labels = np.array(dataset['label'])\n",
        "    num_samples = len(labels)\n",
        "    preds = probs.argmax(axis=-1)\n",
        "    certain = probs.max(axis=-1) > THRESHOLD\n",
        "\n",
        "    num_certain = certain.sum()\n",
        "    num_uncertain = (~certain).sum()\n",
        "\n",
        "    # certain & correct\n",
        "    TP_at_1 = (labels[certain] ==  preds[certain]).sum()\n",
        "    # certain & false\n",
        "    FP_at_1 = (labels[certain] !=  preds[certain]).sum()\n",
        "\n",
        "    # uncertain & correct at top_k\n",
        "    TP_at_k = top_k_accuracy_score(labels[~certain], probs[~certain], k = K, normalize=False, labels=class_ids) \n",
        "\n",
        "    # ucertain & false\n",
        "    FP_at_k = num_uncertain - TP_at_k\n",
        "\n",
        "    # certain & false is worse than uncertain and false(not in top5), but how much? both got wrong.\n",
        "    FP = FP_at_1  + FP_at_k\n",
        "\n",
        "    # few percentage of FP_at_k could be rectified. so add to FP_at_k\n",
        "    # up to UPDATE_SIZE samples. fp_at_k and tp_at_k not differ except latency. \n",
        "    # hyper parameter optimization step(with multi objective) penalize latency there.\n",
        "    TP_at_k += np.max([FP_at_k, UPDATE_SIZE])\n",
        "    \n",
        "    # penalize by K\n",
        "    TP =  TP_at_1 + TP_at_k / K\n",
        "    buzzni = TP / (TP + FP)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
        "    return {\n",
        "        'buzzni': buzzni,\n",
        "        'acc_at_1': acc,\n",
        "        'acc_at_k': top_k_accuracy_score(labels, probs, k = K, normalize=True, labels=class_ids), \n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(datasets, model, human_in_the_loop=False):\n",
        "    model.eval()\n",
        "    batch_size = EVAL_BATCH_SIZE\n",
        "\n",
        "    label_li = []\n",
        "    prop_li = []\n",
        "    \n",
        "    for dataset in datasets:\n",
        "      label_li.append(dataset['label'])\n",
        "      steps, residue = np.divmod(len(dataset), batch_size)\n",
        "      if residue > 0: steps += 1\n",
        "      text_probs_numpy_li = []        \n",
        "\n",
        "      text_features = model.encode_text(text_tokens).float()\n",
        "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "      shape = dataset.shape[0], 50\n",
        "      text_probs_numpy = np.zeros(shape, dtype=np.float16)\n",
        "\n",
        "      for step in tqdm.trange(steps): \n",
        "          image_input = torch.tensor(np.stack(dataset[step*batch_size:(step+1)*batch_size]['image'])).to(device)\n",
        "          image_features = model.encode_image(image_input).float()\n",
        "          image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "          text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "          text_probs_numpy[step*batch_size:(step+1)*batch_size] = text_probs.cpu().numpy().astype(np.float16)\n",
        "        \n",
        "      prop_li.append(text_probs_numpy)\n",
        "\n",
        "    labels = np.concatenate(label_li)\n",
        "    props = np.concatenate(prop_li)\n",
        "    metrics = compute_metrics(labels, props)\n",
        "\n",
        "    if not human_in_the_loop:\n",
        "        return metrics\n",
        "\n",
        "    max_proba = labels.max(axis=-1)\n",
        "    preds = props.argmax(axis=-1)\n",
        "    certain =  max_proba > THRESHOLD\n",
        "\n",
        "    # select most unsure cases.\n",
        "    to_inspect = np.argsort(max_proba)[:UPDATE_SIZE]\n",
        "    certain_index = np.flatnonzero(certain)\n",
        "\n",
        "    # for every epoch. add 10% of update_size(manual labelled) to test_set.\n",
        "    test_size = int(UPDATE_SIZE / 10)\n",
        "    \n",
        "    np.random.seed(0)\n",
        "    to_update = np.random.choice(to_inspect, test_size, replace=False)\n",
        "\n",
        "    to_inspect = np.setdiff1d(to_inspect, to_update)    \n",
        "    \n",
        "    # 하위 100개 중 THRESHOLD 이상인 것들은 제거. 확실한 케이스에 대한 manual inspection 필요를 줄여줌.\n",
        "    to_inspect = np.setdiff1d(to_inspect, certain_index)\n",
        "    to_inspect = np.concatenate([to_inspect, to_update])\n",
        "    \n",
        "    # calculate latency\n",
        "    bad_inspect_time = 1\n",
        "    worse_inspect_time = 2\n",
        "    worst_inspect_time = 10\n",
        "\n",
        "    # uncertain but still correct. human just enther next\n",
        "    uncertain_correct_at_1 = (labels[to_inspect] ==  preds[to_inspect]).sum()\n",
        "    \n",
        "    # uncertain & incorrect. but top k suggestion is correct. human need to click 1/5\n",
        "    uncertain_correct_at_k = top_k_accuracy_score(labels[to_inspect], text_probs_numpy[to_inspect], k = K, normalize=False, labels=class_ids) \n",
        "    uncertain_incorrect_at_k = len(to_inspect) - uncertain_correct_at_k\n",
        "\n",
        "    uncertain_correct_at_k -= uncertain_correct_at_1\n",
        "\n",
        "    bad_inspect_taken = bad_inspect_time * uncertain_correct_at_1\n",
        "    worse_inspect_taken = worse_inspect_time * uncertain_correct_at_k\n",
        "    worst_inspect_taken = worst_inspect_time * uncertain_incorrect_at_k\n",
        "\n",
        "    latency = bad_inspect_taken + worse_inspect_taken + worst_inspect_taken\n",
        "    # uncertain & incorrect. and top k suggestion not contains correct label. human need to manually type correct label.\n",
        "    \n",
        "    # uncertain 데이터의 경우 역설적으로  human이 확인하므로 certain해 져서 train_dataset에 permanently 추가됨\n",
        "    permanent_appendix_dataset = dataset.select(to_inspect)\n",
        "    \n",
        "    # certain의 경우 중간에 human 검수 과정이 들어가지 않으므로 매 epoch마다 다시 계산해서 넣어줌\n",
        "    temporary_appendix_dataset = dataset.select(certain_index)\n",
        "    \n",
        "    # update label to top_1 prediction values\n",
        "    if temporary_appendix_dataset.shape[0] > 0:\n",
        "        temporary_appendix_dataset = (\n",
        "            temporary_appendix_dataset\n",
        "            .remove_columns(\"label\")\n",
        "            .add_column('label', list(preds[certain]))\n",
        "        )\n",
        "\n",
        "    # remove permanent_appendix_dataset from dataset\n",
        "    whole_index = np.arange(text_probs_numpy.shape[0])\n",
        "    keep_index = np.setdiff1d(whole_index, to_inspect)\n",
        "\n",
        "    test_update_dataset = dataset.select(to_update)\n",
        "\n",
        "    # drop manualy labelled data from unlabel dataset\n",
        "    updated_dataset = dataset.select(keep_index)     \n",
        "    return permanent_appendix_dataset, temporary_appendix_dataset, updated_dataset, test_update_dataset, latency\n",
        "            \n",
        "    # _, yhat = text_probs.cpu().topk(1, dim=-1)\n",
        "    # cm = confusion_matrix(ds['label'], yhat.flatten())\n",
        "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    # disp.plot()\n",
        "    # clustering = AgglomerativeClustering(n_clusters=20)\n",
        "    # clustering.fit(normalize(cm, norm='l2', axis=1))\n",
        "    # idx2group = {idx:group  for idx, group in enumerate(clustering.labels_)}\n",
        "    # group2idx = {v:k for k,v in idx2group.items()}\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, ds):\n",
        "        self.ds = ds\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # images = preprocess(self.ds[idx]['image'])\n",
        "        images = torch.tensor(self.ds[idx]['image'])\n",
        "        caption = clip.tokenize(self.ds[idx]['txt'])[0]\n",
        "        return images,caption\n",
        "\n",
        "def convert_models_to_fp32(model): \n",
        "    for p in model.parameters(): \n",
        "        p.data = p.data.float() \n",
        "        p.grad.data = p.grad.data.float() \n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "\n",
        "# torch.dot(loss, torch.from_numpy(sample_weight).float()) / len(y_pred)\n",
        "# model.train()\n",
        "# clip.model.convert_weights(model) # convert to fp16\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params from paper\n",
        "def train_epoch(dl, model):\n",
        "    model.train()\n",
        "    for batch in tqdm.tqdm(dl):\n",
        "        optimizer.zero_grad()\n",
        "        images,texts = batch #list_images is list of image in numpy array(np.uint8)    \n",
        "        logits_per_image, logits_per_text = model(images.to(device), texts.to(device))\n",
        "        # ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
        "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "        total_loss.backward()\n",
        "\n",
        "        convert_models_to_fp32(model)\n",
        "        optimizer.step()\n",
        "        clip.model.convert_weights(model) # back to fp16\n",
        "\n",
        "history = []\n",
        "total_latency = 0\n",
        "lr = 5e-5\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=lr,\n",
        "    betas=(0.9,0.98),\n",
        "    eps=1e-6,\n",
        "    weight_decay=0.2\n",
        "    ) #Params from paper\n",
        "\n",
        "train_dataset = ImageCaptionDataset(ds['train'])\n",
        "# sample_weights = torch.ones(len(train_dataset), dtype=torch.float32)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    # sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True, generator=torch.Generator().manual_seed(42))\n",
        "    ) #Define your own dataloader\n",
        "\n",
        "\n",
        "# save zeroshot result\n",
        "eval_metrics = eval_epoch([ds['eval']], model)\n",
        "# test_metrics = eval_epoch(ds['test'], model)\n",
        "\n",
        "history.append({\n",
        "    'epoch': -1,\n",
        "    'train_size': len(train_dataset), # variadic \n",
        "    'eval_size': len(ds['eval']), # fixed\n",
        "    'unlabel_size': len(ds['unlabel']), # fixed\n",
        "    'test_size': len(ds['test']), # fixed\n",
        "    **{f'eval_{k}':v for k, v in eval_metrics.items()},\n",
        "    # **{f'test_{k}':v for k, v in test_metrics.items()},\n",
        "    'latency': 0,\n",
        "    'total_latency': 0\n",
        "})        \n",
        "\n",
        "\n",
        "test_sets = [ds['test']]\n",
        "for epoch in range(EPOCHS):\n",
        "    _ = train_epoch(train_dataloader, model)\n",
        "\n",
        "    if HUMAN_IN_THE_LOOP and ds['unlabel'].shape[0] > 0:\n",
        "        # append pseudo labeled dataset with human in the loop step(mimic)\n",
        "        permanent_appendix_dataset, temporary_appendix_dataset, updated_dataset, test_update_dataset, latency = eval_epoch( [ds['unlabel']], model, human_in_the_loop=True)\n",
        "        total_latency += latency\n",
        "        # if len(updated): ds['test']['train'] = updated\n",
        "\n",
        "        test_sets.append(test_update_dataset)\n",
        "        ds['unlabel'] = updated_dataset\n",
        "\n",
        "        if len(permanent_appendix_dataset):\n",
        "            permanent_appendix_dataset = ImageCaptionDataset(permanent_appendix_dataset)            \n",
        "            train_dataset = torch.utils.data.ConcatDataset([train_dataset, permanent_appendix_dataset])\n",
        "\n",
        "        temporary_appendix_dataset = ImageCaptionDataset(temporary_appendix_dataset)\n",
        "        temporary_train_dataset = torch.utils.data.ConcatDataset([train_dataset, temporary_appendix_dataset])\n",
        "        train_dataloader = DataLoader(\n",
        "            temporary_train_dataset,\n",
        "            batch_size = BATCH_SIZE,\n",
        "            # sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True, generator=torch.Generator().manual_seed(42))\n",
        "            ) #Define your own dataloader\n",
        "            \n",
        "            # sanity check.\n",
        "        _ = next(iter(train_dataloader))\n",
        "    # save untrained result\n",
        "    eval_metrics = eval_epoch([ds['eval']], model)\n",
        "    test_metrics = eval_epoch(test_sets, model)\n",
        "\n",
        "    history.append({\n",
        "        'epoch': epoch,\n",
        "        'train_size': len(temporary_train_dataset), # variadic \n",
        "        'eval_size': len(ds['eval']), # fixed\n",
        "        'unlabel_size': len(ds['unlabel']), # fixed\n",
        "        'test_size': len(ds['test']), # fixed\n",
        "        **{f'eval_{k}':v for k, v in eval_metrics.items()},\n",
        "        **{f'test_{k}':v for k, v in test_metrics.items()},\n",
        "        'latency': latency,\n",
        "        'total_latency': total_latency,\n",
        "    })        \n",
        "\n",
        "    history_dataframe = pd.DataFrame(history)\n",
        "    print(history_dataframe)\n",
        "dest = Path(catalog.trial_result) \n",
        "dest = dest.parent / f'{dest.stem}_0{dest.suffix}'\n",
        "Path(dest).parent.mkdir(parents=True, exist_ok=True)\n",
        "history_dataframe.to_csv(dest, index=False)\n",
        "best_score = history_dataframe['eval_buzzni'].max()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPac2r8hgipjBrIPKgW+Siv",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "finetune.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12e5a7ddb2824c5b84ffaaeecd1d3724": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e4800d66aec41bc9ebc92203d1c2c23",
              "IPY_MODEL_b3b09fb26d3b4c4c8a0e6ae8179cf915",
              "IPY_MODEL_ce0cfa0cc607498592989082dfb2ef0f"
            ],
            "layout": "IPY_MODEL_fcd497681f7d4bfe9e49b60e8a271252"
          }
        },
        "505e22644f0c406cac79d58f0f5bf546": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f6b1418dcf54e7f874a399d6d0b323b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e4800d66aec41bc9ebc92203d1c2c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1d179b1797b45e9805885330aa079c8",
            "placeholder": "​",
            "style": "IPY_MODEL_e2ea302ea9f74abd83376ab35e968ac8",
            "value": "Flattening the indices: 100%"
          }
        },
        "b3b09fb26d3b4c4c8a0e6ae8179cf915": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f6b1418dcf54e7f874a399d6d0b323b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f05e0d8dd82745869328d6707c00d556",
            "value": 1
          }
        },
        "ce0cfa0cc607498592989082dfb2ef0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_505e22644f0c406cac79d58f0f5bf546",
            "placeholder": "​",
            "style": "IPY_MODEL_db7defadeb6844a08b9374e690d1c237",
            "value": " 1/1 [00:00&lt;00:00, 10.01ba/s]"
          }
        },
        "db7defadeb6844a08b9374e690d1c237": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1d179b1797b45e9805885330aa079c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2ea302ea9f74abd83376ab35e968ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f05e0d8dd82745869328d6707c00d556": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcd497681f7d4bfe9e49b60e8a271252": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
